---
---
@inproceedings{leonardelli-etal-2021-agreeing,
    title = "Agreeing to Disagree: Annotating Offensive Language Datasets with Annotators' Disagreement",
    author = "Leonardelli, Elisa  and
      Menini, Stefano  and
      Palmero Aprosio, Alessio  and
      Guerini, Marco  and
      Tonelli, Sara",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.822/",
    doi = "10.18653/v1/2021.emnlp-main.822",
    pages = "10528--10539",
    abstract = "Since state-of-the-art approaches to offensive language detection rely on supervised learning, it is crucial to quickly adapt them to the continuously evolving scenario of social media. While several approaches have been proposed to tackle the problem from an algorithmic perspective, so to reduce the need for annotated data, less attention has been paid to the quality of these data. Following a trend that has emerged recently, we focus on the level of agreement among annotators while selecting data to create offensive language datasets, a task involving a high level of subjectivity. Our study comprises the creation of three novel datasets of English tweets covering different topics and having five crowd-sourced judgments each. We also present an extensive set of experiments showing that selecting training and test data according to different levels of annotators' agreement has a strong effect on classifiers performance and robustness. Our findings are further validated in cross-domain experiments and studied using a popular benchmark dataset. We show that such hard cases, where low agreement is present, are not necessarily due to poor-quality annotation and we advocate for a higher presence of ambiguous cases in future datasets, in order to train more robust systems and better account for the different points of view expressed online."
}
@inproceedings{leonardelli-etal-2023-semeval,
    title = "{S}em{E}val-2023 Task 11: Learning with Disagreements ({L}e{W}i{D}i)",
    author = "Leonardelli, Elisa  and
      Abercrombie, Gavin  and
      Almanea, Dina  and
      Basile, Valerio  and
      Fornaciari, Tommaso  and
      Plank, Barbara  and
      Rieser, Verena  and
      Uma, Alexandra  and
      Poesio, Massimo",
    editor = {Ojha, Atul Kr.  and
      Do{\u{g}}ru{\"o}z, A. Seza  and
      Da San Martino, Giovanni  and
      Tayyar Madabushi, Harish  and
      Kumar, Ritesh  and
      Sartori, Elisa},
    booktitle = "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.semeval-1.314/",
    doi = "10.18653/v1/2023.semeval-1.314",
    pages = "2304--2318",
    abstract = "NLP datasets annotated with human judgments are rife with disagreements between the judges. This is especially true for tasks depending on subjective judgments such as sentiment analysis or offensive language detection. Particularly in these latter cases, the NLP community has come to realize that the common approach of reconciling' these different subjective interpretations risks misrepresenting the evidence. Many NLP researchers have therefore concluded that rather than eliminating disagreements from annotated corpora, we should preserve themindeed, some argue that corpora should aim to preserve all interpretations produced by annotators. But this approach to corpus creation for NLP has not yet been widely accepted. The objective of the Le-Wi-Di series of shared tasks is to promote this approach to developing NLP models by providing a unified framework for training and evaluating with such datasets. We report on the second such shared task, which differs from the first edition in three crucial respects: (i) it focuses entirely on NLP, instead of both NLP and computer vision tasks in its first edition; (ii) it focuses on subjective tasks, instead of covering different types of disagreements as training with aggregated labels for subjective NLP tasks is in effect a misrepresentation of the data; and (iii) for the evaluation, we concentrated on soft approaches to evaluation. This second edition of Le-Wi-Di attracted a wide array of partici- pants resulting in 13 shared task submission papers."
}
@inproceedings{ramponi-leonardelli-2022-dh,
    title = "{DH}-{FBK} at {S}em{E}val-2022 Task 4: Leveraging Annotators' Disagreement and Multiple Data Views for Patronizing Language Detection",
    author = "Ramponi, Alan  and
      Leonardelli, Elisa",
    editor = "Emerson, Guy  and
      Schluter, Natalie  and
      Stanovsky, Gabriel  and
      Kumar, Ritesh  and
      Palmer, Alexis  and
      Schneider, Nathan  and
      Singh, Siddharth  and
      Ratan, Shyam",
    booktitle = "Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.semeval-1.42/",
    doi = "10.18653/v1/2022.semeval-1.42",
    pages = "324--334",
    abstract = "The subtle and typically unconscious use of patronizing and condescending language (PCL) in large-audience media outlets undesirably feeds stereotypes and strengthens power-knowledge relationships, perpetuating discrimination towards vulnerable communities. Due to its subjective and subtle nature, PCL detection is an open and challenging problem, both for computational methods and human annotators. In this paper we describe the systems submitted by the DH-FBK team to SemEval-2022 Task 4, aiming at detecting PCL towards vulnerable communities in English media texts. Motivated by the subjectivity of human interpretation, we propose to leverage annotators' uncertainty and disagreement to better capture the shades of PCL in a multi-task, multi-view learning framework. Our approach achieves competitive results, largely outperforming baselines and ranking on the top-left side of the leaderboard on both PCL identification and classification. Noticeably, our approach does not rely on any external data or model ensemble, making it a viable and attractive solution for real-world use."
}

@inproceedings{sandri-etal-2023-dont,
    title = "Why Don{'}t You Do It Right? Analysing Annotators' Disagreement in Subjective Tasks",
    author = "Sandri, Marta  and
      Leonardelli, Elisa  and
      Tonelli, Sara  and
      Jezek, Elisabetta",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.178/",
    doi = "10.18653/v1/2023.eacl-main.178",
    pages = "2428--2441",
    abstract = "Annotators' disagreement in linguistic data has been recently the focus of multiple initiatives aimed at raising awareness on issues related to `majority voting' when aggregating diverging annotations. Disagreement can indeed reflect different aspects of linguistic annotation, from annotators' subjectivity to sloppiness or lack of enough context to interpret a text. In this work we first propose a taxonomy of possible reasons leading to annotators' disagreement in subjective tasks. Then, we manually label part of a Twitter dataset for offensive language detection in English following this taxonomy, identifying how the different categories are distributed. Finally we run a set of experiments aimed at assessing the impact of the different types of disagreement on classification performance. In particular, we investigate how accurately tweets belonging to different categories of disagreement can be classified as offensive or not, and how injecting data with different types of disagreement in the training set affects performance. We also perform offensive language detection as a multi-task framework, using disagreement classification as an auxiliary task."
}

@inproceedings{casula-etal-2024-dont,
    title = "Don{'}t Augment, Rewrite? Assessing Abusive Language Detection with Synthetic Data",
    author = "Casula, Camilla  and
      Leonardelli, Elisa  and
      Tonelli, Sara",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.669/",
    doi = "10.18653/v1/2024.findings-acl.669",
    pages = "11240--11247",
    abstract = "Research on abusive language detection and content moderation is crucial to combat online harm. However, current limitations set by regulatory bodies and social media platforms can make it difficult to share collected data. We address this challenge by exploring the possibility to replace existing datasets in English for abusive language detection with synthetic data obtained by rewriting original texts with an instruction-based generative model.We show that such data can be effectively used to train a classifier whose performance is in line, and sometimes better, than a classifier trained on original data. Training with synthetic data also seems to improve robustness in a cross-dataset setting. A manual inspection of the generated data confirms that rewriting makes it impossible to retrieve the original texts online."
}

@inproceedings{trotta-etal-2021-monolingual-cross,
    title = "Monolingual and Cross-Lingual Acceptability Judgments with the {I}talian {C}o{LA} corpus",
    author = "Trotta, Daniela  and
      Guarasci, Raffaele  and
      Leonardelli, Elisa  and
      Tonelli, Sara",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.250/",
    doi = "10.18653/v1/2021.findings-emnlp.250",
    pages = "2929--2940",
    abstract = "The development of automated approaches to linguistic acceptability has been greatly fostered by the availability of the English CoLA corpus, which has also been included in the widely used GLUE benchmark. However, this kind of research for languages other than English, as well as the analysis of cross-lingual approaches, has been hindered by the lack of resources with a comparable size in other languages. We have therefore developed the ItaCoLA corpus, containing almost 10,000 sentences with acceptability judgments, which has been created following the same approach and the same steps as the English one. In this paper we describe the corpus creation, we detail its content, and we present the first experiments on this new resource. We compare in-domain and out-of-domain classification, and perform a specific evaluation of nine linguistic phenomena. We also present the first cross-lingual experiments, aimed at assessing whether multilingual transformer-based approaches can benefit from using sentences in two languages during fine-tuning."
}

@inproceedings{paccosi-etal-2023-scent-sensibility,
    title = "Scent and Sensibility: Perception Shifts in the Olfactory Domain",
    author = "Paccosi, Teresa  and
      Menini, Stefano  and
      Leonardelli, Elisa  and
      Barzon, Ilaria  and
      Tonelli, Sara",
    editor = "Tahmasebi, Nina  and
      Montariol, Syrielle  and
      Dubossarsky, Haim  and
      Kutuzov, Andrey  and
      Hengchen, Simon  and
      Alfter, David  and
      Periti, Francesco  and
      Cassotti, Pierluigi",
    booktitle = "Proceedings of the 4th Workshop on Computational Approaches to Historical Language Change",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.lchange-1.15/",
    doi = "10.18653/v1/2023.lchange-1.15",
    pages = "143--152",
    abstract = "In this work, we investigate olfactory perception shifts, analysing how the description of the smells emitted by specific sources has changed over time. We first create a benchmark of selected smell sources, relying upon existing historical studies related to olfaction. We also collect an English text corpus by retrieving large collections of documents from freely available resources, spanning from 1500 to 2000 and covering different domains. We label such corpus using a system for olfactory information extraction inspired by frame semantics, where the semantic roles around the smell sources in the benchmark are marked. We then analyse how the roles describing Qualities of smell sources change over time and how they can contribute to characterise perception shifts, also in comparison with more standard statistical approaches."
}

@inproceedings{rizzi-etal-2024-soft,
    title = "Soft metrics for evaluation with disagreements: an assessment",
    author = "Rizzi, Giulia  and
      Leonardelli, Elisa  and
      Poesio, Massimo  and
      Uma, Alexandra  and
      Pavlovic, Maja  and
      Paun, Silviu  and
      Rosso, Paolo  and
      Fersini, Elisabetta",
    editor = "Abercrombie, Gavin  and
      Basile, Valerio  and
      Bernadi, Davide  and
      Dudy, Shiran  and
      Frenda, Simona  and
      Havens, Lucy  and
      Tonelli, Sara",
    booktitle = "Proceedings of the 3rd Workshop on Perspectivist Approaches to NLP (NLPerspectives) @ LREC-COLING 2024",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.nlperspectives-1.9/",
    pages = "84--94",
    abstract = "The move towards preserving judgement disagreements in NLP requires the identification of adequate evaluation metrics. We identify a set of key properties that such metrics should have, and assess the extent to which natural candidates for soft evaluation such as Cross Entropy satisfy such properties. We employ a theoretical framework, supported by a visual approach, by practical examples, and by the analysis of a real case scenario. Our results indicate that Cross Entropy can result in fairly paradoxical results in some cases, whereas other measures Manhattan distance and Euclidean distance exhibit a more intuitive behavior, at least for the case of binary classification."
}
@inproceedings{leonardelli-casula-2023-dh,
    title = "{DH}-{FBK} at {S}em{E}val-2023 Task 10: Multi-Task Learning with Classifier Ensemble Agreement for Sexism Detection",
    author = "Leonardelli, Elisa  and
      Casula, Camilla",
    editor = {Ojha, Atul Kr.  and
      Do{\u{g}}ru{\"o}z, A. Seza  and
      Da San Martino, Giovanni  and
      Tayyar Madabushi, Harish  and
      Kumar, Ritesh  and
      Sartori, Elisa},
    booktitle = "Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.semeval-1.261/",
    doi = "10.18653/v1/2023.semeval-1.261",
    pages = "1894--1905",
    abstract = "This paper presents the submissions of the DH-FBK team for the three tasks of Task 10 at SemEval 2023. The Explainable Detection of Online Sexism (EDOS) task aims at detecting sexism in English text in an accurate and explainable way, thanks to a fine-grained annotation that follows a three-level schema: sexist or not (Task A), category of sexism (Task B) and vector of sexism (Task C) exhibited. We use a multi-task learning approach in which models share representations from all three tasks, allowing for knowledge to be shared across them. Notably, with our approach a single model can solve all three tasks. In addition, motivated by the subjective nature of the task, we incorporate inter-annotator agreement information in our multi-task architecture. Although disaggregated annotations are not available, we artificially estimate them using a 5-classifier ensemble, and show that ensemble agreement can be a good approximation of crowd agreement. Our approach achieves competitive results, ranking 32nd out of 84, 24th out of 69 and 11th out of 63 for Tasks A, B and C respectively. We finally show that low inter-annotator agreement levels are associated with more challenging examples for models, making agreement information use ful for this kind of task."
}
@inproceedings{bonetti-etal-2022-work,
    title = "Work Hard, Play Hard: Collecting Acceptability Annotations through a 3{D} Game",
    author = "Bonetti, Federico  and
      Leonardelli, Elisa  and
      Trotta, Daniela  and
      Guarasci, Raffaele  and
      Tonelli, Sara",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.185/",
    pages = "1740--1750",
    abstract = "Corpus-based studies on acceptability judgements have always stimulated the interest of researchers, both in theoretical and computational fields. Some approaches focused on spontaneous judgements collected through different types of tasks, others on data annotated through crowd-sourcing platforms, still others relied on expert annotated data available from the literature. The release of CoLA corpus, a large-scale corpus of sentences extracted from linguistic handbooks as examples of acceptable/non acceptable phenomena in English, has revived interest in the reliability of judgements of linguistic experts vs. non-experts. Several issues are still open. In this work, we contribute to this debate by presenting a 3D video game that was used to collect acceptability judgments on Italian sentences. We analyse the resulting annotations in terms of agreement among players and by comparing them with experts' acceptability judgments. We also discuss different game settings to assess their impact on participants' motivation and engagement. The final dataset containing 1,062 sentences, which were selected based on majority voting, is released for future research and comparisons."
}
@article{munari2022assessing,
  title={Assessing the Influence ERC-funded Research Patented Inventions},
  author={Munari, Federico and Righi, H{\'e}rica and Sobrero, Maurizio and Toschi, Laura and Leonardelli, Elisa and Mainini, Stefano and Tonelli, Sara and Kessler, F Bruno},
  journal={European Research Council},
  year={2022}
}
@article{hauswald2018visual,
  title={A visual cortical network for deriving phonological information from intelligible lip movements},
  author={Hauswald, Anne and Lithari, Chrysa and Collignon, Olivier and Leonardelli, Elisa and Weisz, Nathan},
  journal={Current Biology},
  volume={28},
  number={9},
  pages={1453--1459},
  year={2018},
  publisher={Elsevier}
}
@article{leonardelli2019temporal,
  title={Temporal dynamics of access to amodal representations of category-level conceptual information},
  author={Leonardelli, Elisa and Fait, Elisa and Fairhall, Scott L},
  journal={Scientific reports},
  volume={9},
  number={1},
  pages={239},
  year={2019},
  publisher={Nature Publishing Group UK London}
}
@article{giari2020spatiotemporal,
  title={Spatiotemporal properties of the neural representation of conceptual content for words and pictures--an MEG study},
  author={Giari, Giuliano and Leonardelli, Elisa and Tao, Yuan and Machado, Mayara and Fairhall, Scott L},
  journal={Neuroimage},
  volume={219},
  pages={116913},
  year={2020},
  publisher={Elsevier}
}

@article{leonardelli2015prestimulus,
  title={Prestimulus oscillatory alpha power and connectivity patterns predispose perceptual integration of an audio and a tactile stimulus},
  author={Leonardelli, Elisa and Braun, Christoph and Weisz, Nathan and Lithari, Chrysa and Occelli, Valeria and Zampini, Massimiliano},
  journal={Human brain mapping},
  volume={36},
  number={9},
  pages={3486--3498},
  year={2015},
  publisher={Wiley Online Library}
}

@article{papadelis2012can,
  title={Can magnetoencephalography track the afferent information flow along white matter thalamo-cortical fibers?},
  author={Papadelis, Christos and Leonardelli, Elisa and Staudt, Martin and Braun, Christoph},
  journal={Neuroimage},
  volume={60},
  number={2},
  pages={1092--1105},
  year={2012},
  publisher={Elsevier}
}

@article{leonardelli2022similarity,
  title={Similarity-based fMRI-MEG fusion reveals hierarchical organisation within the brain's semantic system},
  author={Leonardelli, Elisa and Fairhall, Scott L},
  journal={NeuroImage},
  volume={259},
  pages={119405},
  year={2022},
  publisher={Elsevier}
}
@inproceedings{leonardelli2024geography,
  title={The Geography of Information Diffusion in Online Discourse on Europe and Migration},
  author={Leonardelli, Elisa and Tonelli, Sara},
  booktitle={Proceedings of the International AAAI Conference on Web and Social Media},
  volume={18},
  pages={904--916},
  year={2024}
}
@inproceedings{leonardelli2020dh,
  title={DH-FBK@ HaSpeeDe2: Italian Hate Speech Detection via Self-Training and Oversampling.},
  author={Leonardelli, Elisa and Menini, Stefano and Tonelli, Sara},
  booktitle={Proceedings of the Seventh Evaluation Campaign of Natural Language Processing and Speech Tools for Italian. Final Workshop (EVALITA 2020)},
  volume={2765},
  year={2020}
}
@inproceedings{leonardelli2023dh,
  title={DH-FBK at HODI: Multi-Task Learning with Classifier Ensemble Agreement, Oversampling and Synthetic Data},
  author={Leonardelli, Elisa and Casula, Camilla},
  year={2023}
}
@inproceedings{abercrombie20232nd,
  title={2nd Workshop on Perspectivist Approaches to NLP (NLPerspectives 2023)},
  author={Abercrombie, Gavin and Basile, Valerio and Bernardi, Davide and Dudy, Shiran and Frenda, Simona and Havens, Lucy and Leonardelli, Elisa and Tonelli, Sara and others},
  booktitle={CEUR Workshop Proceedings},
  volume={3494},
  pages={1--2},
  year={2023},
  organization={CEUR-WS}
}
@article{leonardelli2012effects,
  title={Effects of looming and static sounds on somatosensory processing: A MEG study},
  author={Leonardelli, Elisa and Occelli, Valeria and Demarchi, Gianpaolo and Grassi, Massimo and Braun, Christoph and Zampini, Massimiliano},
  journal={Seeing and Perceiving},
  volume={25},
  pages={94--94},
  year={2012},
  publisher={Brill}
}